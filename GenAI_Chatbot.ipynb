{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational UI Chatbot App with Gemini, LangChain and Streamlit\n",
        "\n",
        "Here we will build a advanced Conversational UI-based chatbot using LangChain and Streamlit with the following features:\n",
        "\n",
        "- Custom Landing Page\n",
        "- Conversational memory"
      ],
      "metadata": {
        "id": "xPRVq3e03c1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install App and LLM dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.12 -q\n",
        "!pip install langchain-google-genai==0.0.7 -q\n",
        "!pip install langchain-community==0.0.29 -q\n",
        "!pip install streamlit==1.32.2 -q\n",
        "!pip install pyngrok==7.1.5 -q\n",
        "!pip install google-generativeai>=0.3.2 -q"
      ],
      "metadata": {
        "id": "2evPp14fy258"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Gemini API Credentials\n",
        "\n",
        "Here we load it from a file so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Streamlit UI without Memory\n",
        "\n",
        "This simple version shows how to:\n",
        "\n",
        "1. Create a basic Streamlit interface\n",
        "2. Connect directly to Gemini\n",
        "3. Process basic Q&A without memory"
      ],
      "metadata": {
        "id": "-Eio-LoBKnzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile basic_app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "st.title(\"Basic AI Assistant (No Memory)\")\n",
        "\n",
        "# Initialize the LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1)\n",
        "\n",
        "# Simple input/output\n",
        "if user_input := st.chat_input(\"Ask a question\"):\n",
        "    st.chat_message(\"human\").write(user_input)\n",
        "\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = gemini.invoke(user_input)\n",
        "        st.write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBfzThOWKu42",
        "outputId": "1630745a-40d9-4a6e-88b3-907759385a6f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting basic_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the app"
      ],
      "metadata": {
        "id": "6naokMhCMiPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run basic_app.py --server.port=5000 &>./logs.txt &"
      ],
      "metadata": {
        "id": "UZGYWmcfMiPF"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(5000)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248cf744-e2d0-4ae4-9f00-779822aead35",
        "id": "WR65GrUEMrcD"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://0f7c-35-231-3-216.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove running app processes"
      ],
      "metadata": {
        "id": "I_k4wNzQMrcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "NJhCkMhyMrcH"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5135c4eb-ded1-460a-d389-9659db6a2802",
        "id": "LT8xZ404MrcH"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        2554       1  1 09:24 ?        00:00:03 /usr/bin/python3 /usr/local/bin/streamlit run basic_app.py --server.port=5000\n",
            "root        3697       1  0 09:28 ?        00:00:00 /usr/bin/python3 /usr/local/bin/streamlit run manual_app.py --server.port=8989\n",
            "root        4365     694  0 09:30 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        4367    4365  0 09:30 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 2554"
      ],
      "metadata": {
        "id": "kIjWRCybMrcI"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Showing Messages with Manual Memory\n",
        "\n",
        "This demonstrates:\n",
        "\n",
        "1. How to manually implement memory in Streamlit\n",
        "2. The challenge of formatting context for the LLM\n",
        "3. Why specialized tools like LangChain help"
      ],
      "metadata": {
        "id": "-svMcbHaNLpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile manual_app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "st.title(\"AI Assistant with Manual Memory\")\n",
        "\n",
        "# Initialize the LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1)\n",
        "\n",
        "# Initialize session state for memory\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display existing messages\n",
        "for message in st.session_state.messages:\n",
        "    st.chat_message(message[\"role\"]).write(message[\"content\"])\n",
        "\n",
        "# Process new input\n",
        "if user_input := st.chat_input(\"Ask a question\"):\n",
        "    # Add user message to history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": user_input})\n",
        "    st.chat_message(\"human\").write(user_input)\n",
        "\n",
        "    # Manually create a context string from history\n",
        "    context = \"Previous conversation:\\n\"\n",
        "    for msg in st.session_state.messages:\n",
        "        context += f\"{msg['role']}: {msg['content']}\\n\"\n",
        "\n",
        "    with st.chat_message(\"ai\"):\n",
        "        # Send context + new question\n",
        "        full_prompt = context + \"\\nPlease respond to the last question.\"\n",
        "        response = gemini.invoke(full_prompt)\n",
        "        st.write(response.content)\n",
        "\n",
        "        # Add AI response to history\n",
        "        st.session_state.messages.append({\"role\": \"ai\", \"content\": response.content})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgYHo7QwNULa",
        "outputId": "7aa10ef4-dee6-4222-d996-64a040922b3f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting manual_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run manual_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "9bqZgBFiNg_d"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5365a9-5b86-4d5f-931c-5349bab90ab7",
        "id": "ZCA-ShfgNg_e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://0b00-35-231-3-216.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "lX0v5KEeNg_f"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e4f8e8-0ffa-4d4c-9f44-5a8d2893a047",
        "id": "TnlrPbd6Ng_g"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        7173       1  0 09:42 ?        00:00:00 /usr/bin/python3 /usr/local/bin/streamlit run langchain_app.py --server.port=8989\n",
            "root        7559     694  0 09:43 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        7561    7559  0 09:43 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 7173"
      ],
      "metadata": {
        "id": "e46FJ2B9Ng_g"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Full LangChain Integration"
      ],
      "metadata": {
        "id": "RCMshwB1U9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile langchain_app.py\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"AI Assistant\", page_icon=\"🤖\")\n",
        "st.title(\"Welcome I am AI Assistant 🤖\")\n",
        "\n",
        "# Load a connection to Gemini LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1,\n",
        "                               convert_system_message_to_human=True)\n",
        "\n",
        "# Add a basic system prompt for LLM behavior\n",
        "SYS_PROMPT = \"\"\"\n",
        "              Act as a helpful assistant and answer questions to the best of your ability.\n",
        "              Do not make up answers.\n",
        "              \"\"\"\n",
        "\n",
        "# Create a prompt template for langchain to use history to answer user prompts\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "  [\n",
        "    (\"system\", SYS_PROMPT),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Create a basic llm chain\n",
        "llm_chain = (\n",
        "  prompt\n",
        "  |\n",
        "  gemini\n",
        ")\n",
        "\n",
        "# Store conversation history in Streamlit session state\n",
        "streamlit_msg_history = StreamlitChatMessageHistory()\n",
        "\n",
        "# Create a conversation chain\n",
        "conversation_chain = RunnableWithMessageHistory(\n",
        "  llm_chain,\n",
        "  lambda session_id: streamlit_msg_history,  # Accesses memory\n",
        "  input_messages_key=\"input\",\n",
        "  history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# Render current messages from StreamlitChatMessageHistory\n",
        "for msg in streamlit_msg_history.messages:\n",
        "  st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# If user inputs a new prompt, display it and show the response\n",
        "if user_prompt := st.chat_input():\n",
        "  st.chat_message(\"human\").write(user_prompt)\n",
        "  # This is where response from the LLM is shown\n",
        "  with st.chat_message(\"ai\"):\n",
        "    config = {\"configurable\": {\"session_id\": \"any\"}}\n",
        "    # Get llm response\n",
        "    response = conversation_chain.invoke({\"input\": user_prompt}, config)\n",
        "    st.markdown(response.content) # Display response directly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLpP0t_9ojVI",
        "outputId": "8644c3d5-a07a-4601-ab08-8e396f2dba75"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting langchain_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the app"
      ],
      "metadata": {
        "id": "8de1tM6FVLsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run langchain_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "Za_TAI2RkPI9"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "FrVhyQVirAqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82eb08b-8e62-4fbd-aa80-a528b95d6e43"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://d6a1-35-231-3-216.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove running app processes"
      ],
      "metadata": {
        "id": "2Q3yFB_jsgC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "pG7Abg_LrAw6"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "id": "x2VA4ZtCkPNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d109cc4-646e-47ae-9419-f5902a4c38b2"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        7609       1  3 09:44 ?        00:00:04 /usr/bin/python3 /usr/local/bin/streamlit run langchain_app.py --server.port=8989\n",
            "root        8103     694  0 09:45 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        8105    8103  0 09:45 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 7609"
      ],
      "metadata": {
        "id": "_WxGAxGHkPLP"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlBQh5fdkPPY"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}